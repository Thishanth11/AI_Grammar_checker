{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Pdv-zY4ICpFeGKMaQeAYsjeXfDHPB65F",
      "authorship_tag": "ABX9TyODg7YmHV2TdBqd0ujcuMcj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thishanth11/AI_Grammar_checker/blob/main/Grammar_checker_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing"
      ],
      "metadata": {
        "id": "jmVmWFo7gWUO"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/tamil_dataset.csv\")\n",
        "\n",
        "# Print the column names to inspect them\n",
        "print(data.columns)\n",
        "\n",
        "train_sentences, test_sentences, train_targets, test_targets = train_test_split(\n",
        "    data['Input'], data['Target'], test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHLzbxrCfbfp",
        "outputId": "f445378e-0c01-45fe-8c88-b485fa81f5d6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Index(['Input', 'Target'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8zzPFIfrcudN"
      },
      "outputs": [],
      "source": [
        "# Tokenize sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(pd.concat([train_sentences, train_targets]))\n",
        "\n",
        "# Convert text to sequences\n",
        "train_input_seq = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_target_seq = tokenizer.texts_to_sequences(train_targets)\n",
        "test_input_seq = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_target_seq = tokenizer.texts_to_sequences(test_targets)\n",
        "\n",
        "# Pad sequences\n",
        "max_length = max(max(len(seq) for seq in train_input_seq), max(len(seq) for seq in train_target_seq))\n",
        "train_input_seq = pad_sequences(train_input_seq, maxlen=max_length, padding='pre')\n",
        "train_target_seq = pad_sequences(train_target_seq, maxlen=max_length, padding='pre')\n",
        "test_input_seq = pad_sequences(test_input_seq, maxlen=max_length, padding='pre')\n",
        "test_target_seq = pad_sequences(test_target_seq, maxlen=max_length, padding='pre')\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define and Train the Model"
      ],
      "metadata": {
        "id": "9qJ6aSDVgoD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "\n",
        "# Define the encoder\n",
        "encoder_inputs = Input(shape=(max_length,))\n",
        "encoder_embedding = Embedding(vocab_size, 128, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(128, return_state=True, use_cudnn=False)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Define the decoder\n",
        "decoder_inputs = Input(shape=(max_length,))\n",
        "decoder_embedding = Embedding(vocab_size, 128, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm = LSTM(128, return_sequences=True, return_state=True, use_cudnn=False)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Combine into a model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Prepare target data for training\n",
        "train_target_seq = train_target_seq.reshape(train_target_seq.shape[0], train_target_seq.shape[1], 1)\n",
        "\n",
        "# Train the model\n",
        "model.fit([train_input_seq, train_input_seq], train_target_seq, batch_size=32, epochs=25, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUtBwOoXgqcQ",
        "outputId": "8754f646-a7af-493e-89ea-eb4b09f9d8ca"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.0833 - loss: 2.8343 - val_accuracy: 0.2500 - val_loss: 2.8384\n",
            "Epoch 2/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 2.8200 - val_accuracy: 0.2500 - val_loss: 2.8430\n",
            "Epoch 3/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5833 - loss: 2.8054 - val_accuracy: 0.2500 - val_loss: 2.8477\n",
            "Epoch 4/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6667 - loss: 2.7905 - val_accuracy: 0.2500 - val_loss: 2.8525\n",
            "Epoch 5/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.6667 - loss: 2.7748 - val_accuracy: 0.2500 - val_loss: 2.8576\n",
            "Epoch 6/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 2.7580 - val_accuracy: 0.2500 - val_loss: 2.8630\n",
            "Epoch 7/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8333 - loss: 2.7398 - val_accuracy: 0.2500 - val_loss: 2.8687\n",
            "Epoch 8/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8333 - loss: 2.7199 - val_accuracy: 0.2500 - val_loss: 2.8749\n",
            "Epoch 9/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8333 - loss: 2.6979 - val_accuracy: 0.2500 - val_loss: 2.8815\n",
            "Epoch 10/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8333 - loss: 2.6732 - val_accuracy: 0.2500 - val_loss: 2.8887\n",
            "Epoch 11/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8333 - loss: 2.6456 - val_accuracy: 0.2500 - val_loss: 2.8964\n",
            "Epoch 12/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8333 - loss: 2.6143 - val_accuracy: 0.2500 - val_loss: 2.9048\n",
            "Epoch 13/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8333 - loss: 2.5787 - val_accuracy: 0.2500 - val_loss: 2.9140\n",
            "Epoch 14/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8333 - loss: 2.5381 - val_accuracy: 0.2500 - val_loss: 2.9240\n",
            "Epoch 15/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8333 - loss: 2.4916 - val_accuracy: 0.2500 - val_loss: 2.9349\n",
            "Epoch 16/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8333 - loss: 2.4383 - val_accuracy: 0.2500 - val_loss: 2.9469\n",
            "Epoch 17/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8333 - loss: 2.3772 - val_accuracy: 0.2500 - val_loss: 2.9599\n",
            "Epoch 18/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8333 - loss: 2.3073 - val_accuracy: 0.2500 - val_loss: 2.9743\n",
            "Epoch 19/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8333 - loss: 2.2277 - val_accuracy: 0.2500 - val_loss: 2.9900\n",
            "Epoch 20/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6667 - loss: 2.1377 - val_accuracy: 0.2500 - val_loss: 3.0073\n",
            "Epoch 21/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.6667 - loss: 2.0376 - val_accuracy: 0.2500 - val_loss: 3.0263\n",
            "Epoch 22/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6667 - loss: 1.9286 - val_accuracy: 0.2500 - val_loss: 3.0473\n",
            "Epoch 23/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6667 - loss: 1.8135 - val_accuracy: 0.2500 - val_loss: 3.0706\n",
            "Epoch 24/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6667 - loss: 1.6966 - val_accuracy: 0.2500 - val_loss: 3.0963\n",
            "Epoch 25/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6667 - loss: 1.5831 - val_accuracy: 0.2500 - val_loss: 3.1249\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78f946f76830>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Evaluate the Model"
      ],
      "metadata": {
        "id": "miIgHDzGgzRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test data\n",
        "test_target_seq = test_target_seq.reshape(test_target_seq.shape[0], test_target_seq.shape[1], 1)\n",
        "loss, accuracy = model.evaluate([test_input_seq, test_input_seq], test_target_seq)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNpDyqfIg1n6",
        "outputId": "17f96a8f-cc27-4510-e12e-7a4931f82cc0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step - accuracy: 0.7500 - loss: 0.0000e+00\n",
            "Test Accuracy: 75.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict corrected sentences\n",
        "def predict_sentence(sentence):\n",
        "    seq = tokenizer.texts_to_sequences([sentence])\n",
        "    seq = pad_sequences(seq, maxlen=max_length, padding='post')\n",
        "    prediction = model.predict([seq, seq])\n",
        "    predicted_sequence = tf.argmax(prediction[0], axis=-1).numpy()\n",
        "    predicted_sentence = \" \".join([tokenizer.index_word.get(idx, \"\") for idx in predicted_sequence if idx != 0])\n",
        "    return predicted_sentence\n",
        "\n",
        "# Test with user input\n",
        "user_sentence = input(\"Enter a Tamil sentence: \")\n",
        "corrected_sentence = predict_sentence(user_sentence)\n",
        "print(\"Corrected Sentence:\", corrected_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvugMbpZis9l",
        "outputId": "adfe4528-5d7c-4cb1-c381-dfbc85f53d1a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a Tamil sentence: பார்த்து நான் புத்தகம் வாங்கினேன.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step\n",
            "Corrected Sentence: சமைக்கிறார்\n"
          ]
        }
      ]
    }
  ]
}